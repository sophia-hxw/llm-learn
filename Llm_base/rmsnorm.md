# 层归一化
## 基础知识
假设我们有一个输入向量  $\pmb x=[𝑥_1, 𝑥_2,\cdots ,𝑥_H]$ 表示某一层的激活值，其中 $H$ 是该层的单元（神经元）数。

1. 计算均值 
$$ u:=\frac{1}{H}\sum^H_{i=1}x_i ​$$
 
2. 计算方差 
$$ \sigma^2=\frac{1}{H}\sum^H_{i=1}(x_i-u)^2 $$
​

3. 归一化
将每个激活值归一化为零均值、单位方差的形式：
$$ \hat x_i =\frac{x_i-u}{\sqrt{\sigma^2+\epsilon}} $$
其中，$\epsilon$ 是一个小的常数，用于防止分母为零。

4. 缩放和平移
最后，应用可学习的参数 $\gamma$ 和 $\beta$ 对归一化后的激活值进行缩放和平移：
$$ y_i=\gamma \hat x_i+\beta $$

其中，$\gamma$ 和 $\beta$ 是与每个神经元对应的可学习参数，分别用于调整输出的尺度和偏移。

最终输出：
$$ \pmb y=[y_1,y_2,\cdots,y_H] $$
通过这个过程，层归一化能够对每一层的激活值进行归一化，从而提高神经网络的训练效率和稳定性。

## 为什么管用
1. 减轻内部协变量偏移
层归一化能够减轻或消除内部协变量偏移问题，即模型训练过程中，输入数据的分布不断变化导致模型难以收敛。通过对每一层的激活值进行归一化，层归一化可以确保每一层的输入在一定范围内稳定，从而加快训练过程。

2. 加快收敛速度
层归一化通过标准化激活值，可以减少不同神经元之间的依赖性，使得梯度在反向传播时更加平滑。这有助于加快优化过程，特别是在深度网络中。

3. 减少对批量大小的依赖
与批量归一化（Batch Normalization）不同，层归一化在处理小批量数据或单个样本时表现更好，因为它不依赖于整个批次的数据统计。它在训练过程中可以应用于每个样本，而不需要批量数据的统计信息。

4. 提高模型泛化能力
层归一化在不同层中应用归一化，能够提高模型的泛化能力，使其在处理未见过的数据时表现更好。

# RMSNorm

它是层归一化（Layer Normalization）的变体。RMSNorm通过简化层归一化中的计算步骤，保持了归一化的效果，同时降低了计算复杂度。

## 原理
RMSNorm与Layer Norm的主要区别在于，它不使用均值来归一化，而是直接利用均方根（RMS，Root Mean Square）来归一化输入。具体来说，RMSNorm的主要计算步骤如下：

1. 计算均方根（RMS）：
   假设我们有一个输入向量 $\pmb x=[𝑥_1, 𝑥_2,\cdots ,𝑥_H]$，首先计算该向量的均方根：
   $$ RMS(x)=\sqrt{\frac{1}{H}\sum^H_{i=1}x^2_i} $$
2. 归一化：
    然后，将每个激活值归一化为均方根形式：
    $$\hat x_i=\frac{x_i}{RMS(x)+\epsilon} $$
    其中，$\epsilon$ 是一个小的常数，用于防止分母为零。
3. 缩放平移和输出与层归一化方法一致

## 优势
计算效率高：相比于Layer Norm需要计算均值和方差，RMSNorm只计算均方根，减少了计算量，因此更高效。

稳定性：尽管不计算均值，RMSNorm仍能稳定模型的训练过程，特别适用于深度神经网络中的大规模模型。

适用于各种网络架构：RMSNorm可以应用于各种类型的神经网络，包括前馈神经网络、卷积神经网络（CNNs）、以及递归神经网络（RNNs）等。

## 应用场景
RMSNorm可以用于那些需要高效归一化操作的深度学习模型中，特别是当计算资源有限或需要降低模型的计算复杂度时。例如，在大规模自然语言处理模型（如Transformer）的训练中，RMSNorm已经显示出比传统的Layer Norm更好的性能和效率。

