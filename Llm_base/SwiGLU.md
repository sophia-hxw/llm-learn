# SwiGLU
## 是什么？
SwiGLU（Swish-Gated Linear Unit）是一种激活函数，它是在深度学习模型中用于替代标准激活函数（如ReLU或Swish）的新型激活函数。SwiGLU 是 GLU（Gated Linear Unit）和 Swish 激活函数的结合，旨在增强模型的表现能力和训练效率。

## 构成
SwiGLU 激活函数的基本形式可以表示为：
$$ SwiGLU(x) = Swish(x) \otimes GLU(x) $$
其中：
Swish 是一种流行的激活函数，由 Google 提出，其定义为：
$$ Swish(x) = x\cdot\sigma (x) $$
这里的 $ \sigma (x) $ 是 Sigmoid 函数，定义为：
$$ \sigma(x) = \frac{1}{1+e^{-x}} $$
Swish 函数具有平滑的非线性特性，并且能够在深度网络中表现出更好的性能。

GLU（Gated Linear Unit） 是一种基于门控机制的激活函数，通常用于控制信息的流动。其定义为：
$$ GLU(x,v) = x\cdot \sigma(v) $$
其中，$x$ 是输入，$v$ 是门控向量，$ \sigma (x) $ 是 Sigmoid 函数。

在 SwiGLU 中，Swish 和 GLU 结合在一起，形成一个更强大的激活机制。

## SwiGLU的公式
SwiGLU 的具体实现可以表达为：
$$ SwiGLU(x) = (xW_1+b_1)\otimes Swish(xW_2+b_2) $$

其中：
$x$ 是输入张量；
$W_1$  和 $W_2$ 是线性变换的权重矩阵；
$b_1$  和 $b_2$ 是偏置向量;

## 为什么好
1、更好的表现能力：通过结合 Swish 和 GLU 的优点，SwiGLU 提供了更复杂的非线性变换，能够捕捉更复杂的模式，进而提升模型的表现能力。（如：Swish对于负值的响应相对较小克服了 ReLU 某些神经元上输出始终为零的缺点，Swish 的平滑非线性特性使得 SwiGLU 在梯度传播过程中更加稳定，有助于模型训练。）

2、GLU 的门控特性，这意味着它可以根据输入的情况决定哪些信息应该通过、哪些信息应该被过滤。这种机制可以使网络更有效地学习到有用的表示，增强了激活函数的表征能力，有助于提高模型的泛化能力。在大语言模型中，这对于处理长序列、长距离依赖的文本特别有用。

3、SwiGLU 中的参数 W1,W2,W3,b1,b2,b3W1,W2,W3,b1,b2,b3 可以通过训练学习，使得模型可以根据不同任务和数据集动态调整这些参数，增强了模型的灵活性和适应性。

4、计算效率相比某些较复杂的激活函数（如 GELU）更高，同时仍能保持较好的性能。这对于大规模语言模型的训练和推理是很重要的考量因素。

选择 SwiGLU  作为大语言模型的激活函数，主要是因为它综合了非线性能力、门控特性、梯度稳定性和可学习参数等方面的优势。在处理语言模型中复杂的语义关系、长依赖问题、以及保持训练稳定性和计算效率方面，SwiGLU 表现出色，因此被广泛采用。

# GLU
最早在自然语言处理（NLP）领域被引入，旨在通过引入门控机制来增强神经网络的非线性表达能力。GLU 由 Yann Dauphin 等人在论文《Language Modeling with Gated Convolutional Networks》中提出。它是门控机制（Gating Mechanism）的一种应用，常用于改善网络的学习效率和性能。

## GLU激活函数的定义
GLU 的基本形式可以表示为：
$$ GLU(x,v)=(xW+b)\otimes (xV+c) $$
其中：
$x$ 是输入张量。
$W$ 和 $V$ 是可学习的权重矩阵。
$b$ 和 $c$ 是偏置向量。
$\sigma$ 是 Sigmoid 函数，定义为:

$$ \sigma (z)=\frac{1}{1+e^{-z}} $$

在 GLU 中，输入 $x$ 通过两个线性变换得到两个向量。一个是线性输出 $(xW+b)$，另一个是门控向量 $(xV+c)$ 的 Sigmoid 变换。最终的输出是这两个向量的逐元素乘积。

## GLU的工作机制
GLU 可以理解为在普通的线性变换之后引入了一个门控机制，来控制哪些信息可以通过，哪些信息会被抑制。这种门控机制有点类似于 LSTM（长短期记忆网络）中的门控结构，它通过 Sigmoid 函数对信息进行选择性地传递。

>门控机制（Gate Mechanism）：门控向量通过 Sigmoid 函数生成的值在 0 到 1 之间，起到了“开关”的作用。门控向量中的每个元素会决定对应位置的输入是被保留还是被抑制。

## 为什么使用GLU？
增强非线性表达能力：相比于传统的线性变换，GLU 通过门控机制引入了额外的非线性，使得模型能够学习到更加复杂的模式。

选择性信息传递：门控机制能够选择性地传递信息，这样可以避免无关信息对后续层的干扰，提升模型的学习效率。

减轻梯度消失问题：GLU 中的 Sigmoid 函数在一定程度上有助于缓解梯度消失问题，特别是在深层神经网络中。

## GLU的应用场景
GLU 激活函数主要应用于需要处理序列数据的模型中，特别是在自然语言处理任务中。例如，GLU 被用在了基于卷积网络的语言模型和其他序列建模任务中。以下是一些应用场景：

自然语言处理：在语言模型中，GLU 可以通过门控机制更好地捕捉句子中的长程依赖关系。
卷积神经网络（CNNs）：GLU 在 CNNs 中应用时，能够提升卷积层的非线性表达能力，增强特征提取效果。
序列建模：在处理时序数据或其他序列任务时，GLU 能够帮助模型更好地选择性地传递重要信息，抑制无关信息。

## GLU的变种
GLU 的基本思想已被扩展到其他门控激活函数中，例如：

Gated ReLU (ReGLU)：ReLU 和 GLU 的结合。
Gated Tanh (TanhGLU)：Tanh 和 GLU 的结合。
SwiGLU：结合 Swish 和 GLU，以获得更平滑和强大的激活效果。
